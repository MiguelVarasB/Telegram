"""
ETAPA INDEXAR - M√≥dulo de indexaci√≥n de archivos multimedia de Telegram
========================================================================

Este archivo es responsable de escanear, indexar y relacionar archivos multimedia 
(descargados de Telegram) con sus mensajes originales en la base de datos de Unigram.

FUNCIONALIDADES PRINCIPALES:
----------------------------
1. Escanea carpetas en busca de archivos nuevos (im√°genes, videos, .bin)
2. Extrae IDs num√©ricos de los nombres de archivo (patr√≥n 15-20 d√≠gitos)
3. Busca coincidencias en la base de datos cifrada de Unigram mediante:
   - B√∫squeda binaria en blobs de datos (m√©todo principal)
   - B√∫squeda por nombre de archivo (fallback)
4. Almacena metadatos en tabla local 'cacheo': tama√±o, duraci√≥n, canal_id, msg_id
5. Maneja archivos sin patr√≥n num√©rico (videos sin ID)

FLUJO DE TRABAJO:
------------------
1. iter_archivos_nuevos() ‚Üí Identifica archivos no procesados
2. procesar_archivos() ‚Üí Procesa cada archivo y lo relaciona con mensajes
3. run_etapa_indexar() ‚Üí Punto de entrada principal

BASES DE DATOS UTILIZADAS:
--------------------------
- DB_UNIGRAM (sqlcipher): Base cifrada de Unigram con mensajes originales
- DB local (cacheo.sqlite): Tabla 'cacheo' con resultados de indexaci√≥n
"""

import os
import struct
import re
import datetime
from typing import Iterable

from sqlcipher3 import dbapi2 as sqlcipher

from .common import (
    CARPETAS,
    MASTER_KEY,
    DB_UNIGRAM,
    preparar_base_local,
    obtener_duracion_video,
    cargar_mensajes_unigram,
)
from utils import  log_timing

def iter_archivos_nuevos(archivos_ya_indexados: set) -> list[dict]:
    """
    Escanea las carpetas configuradas en busca de archivos nuevos que no han sido indexados.
    
    Args:
        archivos_ya_indexados: Conjunto de nombres de archivos ya procesados
        
    Returns:
        Lista de diccionarios con informaci√≥n de archivos nuevos, ordenados por fecha de creaci√≥n (m√°s nuevos primero)
        
    Estructura del diccionario devuelto:
        {
            "nombre": str,           # Nombre del archivo
            "tipo": str,            # Tipo seg√∫n CARPETAS (ej: "video", "imagen")
            "ruta": str,            # Ruta completa al archivo
            "fecha_creacion": float # Timestamp de creaci√≥n
        }
    """
    # Extensiones de archivos multimedia que Telegram descarga
    extensiones = ('.jpg', '.png', '.mp4', '.m4v', '.mov', '.bin')
    lista = []
    
    # Recorrer todas las carpetas configuradas en common.py
    for tipo, ruta in CARPETAS.items():
        if not os.path.exists(ruta):
            continue  # Saltar carpetas que no existen
            
        # Escanear archivos en la carpeta
        for f in os.listdir(ruta):
            # Incluir: archivos con extensiones v√°lidas O archivos que son solo n√∫meros (IDs)
            if f.lower().endswith(extensiones) or f.isdigit():
                # Procesar solo si no est√° indexado
                if f not in archivos_ya_indexados:
                    full_path = os.path.join(ruta, f)
                    lista.append(
                        {
                            "nombre": f,
                            "tipo": tipo,
                            "ruta": full_path,
                            "fecha_creacion": os.path.getctime(full_path),
                        }
                    )
    
    # Ordenar por fecha de creaci√≥n (m√°s nuevos primero) para priorizar contenido reciente
    lista.sort(key=lambda x: x["fecha_creacion"], reverse=True)
    return lista


def _buscar_en_unigram_por_nombre(nombre_archivo: str, cur_unigram) -> tuple[int | None, int | None]:
    """
    M√âTODO FALLBACK: Busca un archivo en la base de datos de Unigram por su nombre exacto.
    
    Este m√©todo se utiliza cuando la b√∫squeda binaria por ID no encuentra coincidencias.
    Busca el nombre del archivo dentro de los blobs de datos de los mensajes.
    
    Args:
        nombre_archivo: Nombre del archivo a buscar
        cur_unigram: Cursor de la base de datos de Unigram ya abierta
        
    Returns:
        Tupla (dialog_id, message_id_real) si encuentra coincidencia, o (None, None) si no
        
    Nota:
        - message_id_real = message_id // 1048576 (conversi√≥n de ID interno de Unigram)
        - Este m√©todo es m√°s lento que la b√∫squeda binaria pero √∫til como fallback
    """
    nombre_bytes = nombre_archivo.encode("utf-8")
    
    # Buscar en todos los mensajes que tengan datos binarios
    cur_unigram.execute("SELECT dialog_id, message_id, data FROM messages WHERE data IS NOT NULL")
    
    for d_id, m_id, blob in cur_unigram:
        # Verificar si el nombre del archivo est√° contenido en el blob de datos
        if nombre_bytes in blob:
            # Convertir el ID de mensaje de Unigram a ID global real
            msg_id_real = m_id // 1048576
            return d_id, msg_id_real
            
    return None, None


def procesar_archivos(lista_a_procesar: Iterable[dict]) -> None:
    """
    FUNCI√ìN PRINCIPAL: Procesa una lista de archivos y los relaciona con mensajes de Telegram.
    
    Esta funci√≥n realiza el trabajo pesado de indexaci√≥n:
    1. Extrae IDs num√©ricos de los nombres de archivo
    2. Busca coincidencias en la base de datos de Unigram
    3. Extrae metadatos (tama√±o, duraci√≥n para videos)
    4. Guarda resultados en la tabla local 'cacheo'
    5. Aplica fallback por nombre si la b√∫squeda binaria falla
    
    Args:
        lista_a_procesar: Iterable de diccionarios con informaci√≥n de archivos (de iter_archivos_nuevos)
        
    Proceso detallado:
        - Para cada archivo: extraer ID ‚Üí buscar en blobs ‚Üí guardar metadatos ‚Üí fallback si es necesario
        - Usa commits en lotes de 100 para optimizar rendimiento
        - Maneja archivos sin patr√≥n num√©rico (videos sin ID)
    """
    # Preparar conexi√≥n a base de datos local y obtener archivos ya indexados
    conn_local, archivos_ya_indexados = preparar_base_local()
    nuevos_hallazgos = 0
    batch_size = 100  # Tama√±o del lote para commits a BD
    pendientes = 0
    saltados_patron: list[str] = []  # Archivos que no cumplen el patr√≥n de ID num√©rico

    # Cargar todos los mensajes cifrados de Unigram en memoria para b√∫squeda eficiente
    conn_unigram, todos_los_mensajes = cargar_mensajes_unigram()
    log_timing(f"‚úÖ {len(todos_los_mensajes)} mensajes cargados.")

    # Conexi√≥n adicional para fallback por nombre (se mantiene abierta para m√∫ltiples b√∫squedas)
    conn_uni_nom = sqlcipher.connect(DB_UNIGRAM)
    cur_uni_nom = conn_uni_nom.cursor()
    cur_uni_nom.execute(f"PRAGMA key = \"x'{MASTER_KEY}'\";")  # Descifrar BD
    cur_uni_nom.execute("PRAGMA cipher_compatibility = 4;")    # Compatibilidad con versi√≥n antigua

    try:
        cur_local = conn_local.cursor()
        total_items = len(lista_a_procesar)
        
        # Procesar cada archivo en la lista
        for idx, item in enumerate(lista_a_procesar, start=1):
            nombre_f = item["nombre"]
            
            # Mostrar progreso cada 50 archivos o al final
            if idx % 50 == 0 or idx == total_items:
                log_timing(f"üîé Procesando {idx}/{total_items}: {nombre_f}")

            # EXTRAER ID NUM√âRICO: Buscar n√∫mero de 15-20 d√≠gitos en el nombre del archivo
            # Este ID es la huella digital que permite relacionar el archivo con su mensaje
            match = re.search(r"(\d{15,20})", nombre_f)
            if not match:
                # CASO 1: Archivo sin ID num√©rico (videos descargados sin n√∫mero)
                if item["tipo"] == "video":
                    # Extraer metadatos b√°sicos del video
                    tamano_bytes = os.path.getsize(item["ruta"])
                    duracion_segundos = obtener_duracion_video(item["ruta"])
                    
                    # Guardar en BD como video no relacionado (encontrado=0)
                    cur_local.execute(
                        """
                        INSERT OR IGNORE INTO cacheo
                        (archivo, tipo, fecha_escaneo, encontrado, canal_id, msg_id_global, tamano_bytes, duracion_segundos, en_servidor, unique_id)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        """,
                        (
                            nombre_f,
                            item["tipo"],
                            datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                            0,          # encontrado=0 (no se encontr√≥ relaci√≥n)
                            None,       # canal_id desconocido
                            None,       # msg_id_global desconocido
                            tamano_bytes,
                            duracion_segundos,
                            0,          # en_servidor=0
                            None,       # unique_id desconocido
                        ),
                    )
                    pendientes += 1
                    nuevos_hallazgos += 1
                    
                    # Commit parcial cada batch_size registros
                    if pendientes >= batch_size:
                        conn_local.commit()
                        log_timing(f"üíæ Guardados {nuevos_hallazgos} registros (commit parcial)...")
                        pendientes = 0
                else:
                    # Archivos no videos sin ID se omiten (im√°genes sin n√∫mero no son √∫tiles)
                    saltados_patron.append(nombre_f)
                continue

            # CASO 2: Archivo con ID num√©rico - procesamiento normal
            id_cache_num = int(match.group(1))
            huella_bin = struct.pack("<q", id_cache_num)  # Convertir ID a binario little-endian

            # Extraer metadatos del archivo
            tamano_bytes = os.path.getsize(item["ruta"])
            duracion_segundos = None
            if item["tipo"] == "video":
                duracion_segundos = obtener_duracion_video(item["ruta"])

            # Estructura de informaci√≥n a guardar en BD
            info = {
                "archivo": nombre_f,
                "tipo": item["tipo"],
                "fecha_escaneo": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "encontrado": 0,        # Por defecto no encontrado
                "canal_id": None,
                "msg_id_global": None,
                "tamano_bytes": tamano_bytes,
                "duracion_segundos": duracion_segundos,
            }

            # B√öSQUEDA PRINCIPAL: Buscar la huella binaria en los mensajes de Unigram
            for m_id, d_id, data_blob in todos_los_mensajes:
                if data_blob and huella_bin in data_blob:
                    # ¬°Coincidencia encontrada! Actualizar informaci√≥n
                    info["encontrado"] = 1
                    info["canal_id"] = d_id
                    info["msg_id_global"] = m_id // 1048576  # Convertir a ID real
                    break

            # Guardar en base de datos local
            cur_local.execute(
                """
                INSERT OR IGNORE INTO cacheo
                (archivo, tipo, fecha_escaneo, encontrado, canal_id, msg_id_global, tamano_bytes, duracion_segundos, en_servidor, unique_id)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """,
                (
                    info["archivo"],
                    info["tipo"],
                    info["fecha_escaneo"],
                    info["encontrado"],
                    info["canal_id"],
                    info["msg_id_global"],
                    info["tamano_bytes"],
                    info["duracion_segundos"],
                    0,          # en_servidor=0
                    None,       # unique_id desconocido
                ),
            )
            pendientes += 1
            nuevos_hallazgos += 1

            # Commit parcial cada batch_size registros
            if pendientes >= batch_size:
                conn_local.commit()
                log_timing(f"üíæ Guardados {nuevos_hallazgos} registros (commit parcial)...")
                pendientes = 0

            # Mostrar √©xito si se encontr√≥ relaci√≥n
            if info["encontrado"]:
                log_timing(
                    f"  ‚úÖ {info['tipo'][:3].upper()} {nombre_f} -> Global: {info['msg_id_global']} (Canal: {info['canal_id']})"
                )
            
            # FALLBACK: Si es video y no se encontr√≥ por blob, intentar b√∫squeda por nombre
            if info["tipo"] == "video" and not info["encontrado"]:
                chat_id_f, msg_id_f = _buscar_en_unigram_por_nombre(nombre_f, cur_uni_nom)
                if chat_id_f is not None and msg_id_f is not None:
                    # Actualizar el registro con la informaci√≥n encontrada por fallback
                    cur_local.execute(
                        """
                        UPDATE cacheo
                        SET encontrado = 1, canal_id = ?, msg_id_global = ?
                        WHERE archivo = ?
                        """,
                        (chat_id_f, msg_id_f, info["archivo"]),
                    )
                    info["encontrado"] = 1
                    info["canal_id"] = chat_id_f
                    info["msg_id_global"] = msg_id_f
                    log_timing(f"  ‚úÖ Fallback nombre -> Global: {msg_id_f} (Canal: {chat_id_f}) para {nombre_f}")

        # Commit final de los registros pendientes
        if pendientes:
            conn_local.commit()

        # Resumen final del procesamiento
        log_timing(f"\n‚ú® ¬°Hecho! Se agregaron {nuevos_hallazgos} entradas a cacheo (ignorando duplicados).")
        if saltados_patron:
            log_timing(f"‚ö†Ô∏è Saltados por no cumplir patr√≥n (sin n√∫mero 15-20 d√≠gitos): {len(saltados_patron)}")
            for name in saltados_patron[:30]:  # Mostrar primeros 30
                log_timing(f"   - {name}")
            if len(saltados_patron) > 30:
                log_timing("   ...")
                
    finally:
        # Cerrar todas las conexiones a bases de datos
        conn_local.close()
        conn_unigram.close()
        conn_uni_nom.close()


def run_etapa_indexar():
    """
    PUNTO DE ENTRADA PRINCIPAL del m√≥dulo de indexaci√≥n.
    
    Esta funci√≥n orquesta todo el proceso de indexaci√≥n:
    1. Verifica archivos ya procesados en la base de datos local
    2. Identifica archivos nuevos en las carpetas configuradas
    3. Inicia el procesamiento de los archivos encontrados
    
    Es la funci√≥n que se debe llamar para ejecutar una nueva ronda de indexaci√≥n.
    """
    # Obtener conexi√≥n y lista de archivos ya indexados
    conn_local, existentes = preparar_base_local()
    conn_local.close()
    
    # Identificar archivos nuevos que no han sido procesados
    lista = iter_archivos_nuevos(existentes)
    
    if not lista:
        log_timing("‚òï No hay archivos nuevos que procesar.")
        return
        
    # Iniciar procesamiento de los archivos nuevos
    log_timing(f"üöÄ Analizando {len(lista)} archivos nuevos...")
    procesar_archivos(lista)


if __name__ == "__main__":
    """
    Permite ejecutar este m√≥dulo directamente desde l√≠nea de comandos:
    python etapa_indexar.py
    """
    run_etapa_indexar()
